import torch
from torch import nn, optim


# Custom module type that implements useful methods to export hyperparameters,
# optimizers and loss functions. This is inspired by pytorch lightning and allows
# for slightly cleaner code, as criterions and loss functions do not need to be
# passed around as much.
class CustomModuleType(nn.Module):
    @property
    def hyperparameters(self) -> dict:
        """List of hyperparameters to use to reconstruct the model after training."""
        ...

    @property
    def criterion(self) -> nn.Module:
        """The loss function to use to train the model."""
        ...

    @property
    def optimizer(self) -> optim.Optimizer:
        """The optimizer to use to train the model."""
        ...


class WFDefectDetector(CustomModuleType):
    def __init__(
        self,
        kernel_size_conv: int = 3,
        kernel_size_pool: int = 2,
        num_base_channels: int = 16,
        num_conv_blocks: int = 2,
        activation: str = "relu",
        optimizer: str = "adam",
        learning_rate: float = 0.001,
    ) -> None:
        super().__init__()
        # Perform validation on the hyperparameters. Others may technically be used but
        # these values fit our general idea for the model
        assert kernel_size_conv in [3, 5, 7]
        assert kernel_size_pool in [2, 3]
        assert num_base_channels in [8, 16, 32]
        assert num_conv_blocks in [2, 3]
        assert activation in ["tanh", "relu", "leakyrelu"]
        assert optimizer in ["sgd", "rmsprop", "adam"]

        # Save hyperparameters for exporting by the hyperparameters property
        self.hyperparams = {
            "kernel_size_conv": kernel_size_conv,
            "kernel_size_pool": kernel_size_pool,
            "num_base_channels": num_base_channels,
            "num_conv_blocks": num_conv_blocks,
            "activation": activation,
            "optimizer": optimizer,
            "learning_rate": learning_rate,
        }

        # Get activation class object from string
        if activation == "tanh":
            activation_class = nn.Tanh
        elif activation == "relu":
            activation_class = nn.ReLU
        elif activation == "leakyrelu":
            activation_class = nn.LeakyReLU

        # Get optimizer class object from string
        if optimizer == "sgd":
            optimizer_class = optim.SGD
        elif optimizer == "rmsprop":
            optimizer_class = optim.RMSprop
        elif optimizer == "adam":
            optimizer_class = optim.Adam

        # Save hyperparameters used by the forward method and optimizer property
        self.num_conv_blocks = num_conv_blocks
        self.learning_rate = learning_rate
        self.optimizer_class = optimizer_class

        # Compute padding such that spatial resolution remains the same after convolving
        padding = int((kernel_size_conv - 1) / 2)

        # First convolution block
        self.conv1 = nn.Conv2d(
            in_channels=3,
            out_channels=num_base_channels,
            kernel_size=kernel_size_conv,
            padding=padding,
        )
        self.act1 = activation_class()
        self.maxpool1 = nn.MaxPool2d(kernel_size=kernel_size_pool, stride=2)

        # Second convolution block
        self.conv2 = nn.Conv2d(
            in_channels=num_base_channels,
            out_channels=2 * num_base_channels,
            kernel_size=kernel_size_conv,
            padding=padding,
        )
        self.act2 = activation_class()
        self.maxpool2 = nn.MaxPool2d(kernel_size=kernel_size_pool, stride=2)

        # Optional third convolutional block
        if num_conv_blocks == 3:
            self.conv3 = nn.Conv2d(
                in_channels=2 * num_base_channels,
                out_channels=4 * num_base_channels,
                kernel_size=kernel_size_conv,
                padding=padding,
            )
            self.act3 = activation_class()
            self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Dense prediction head. Lazy layer is used to automatically infer flattened
        # size to prevent having to perform difficult size calculations
        self.flatten = nn.Flatten()
        self.fc1 = nn.LazyLinear(out_features=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Simple forward method passing through all defined layers in order.

        Args:
            x (torch.Tensor): batch of input images of size (N, C, H, W).

        Returns:
            torch.Tensor: batch of probabilities of size (N,). The output is squeezed to
                align with the target batches generated by the ImageFolder dataloaders.
        """
        out = self.maxpool1(self.act1(self.conv1(x)))
        out = self.maxpool2(self.act2(self.conv2(out)))
        if self.num_conv_blocks == 3:
            out = self.maxpool3(self.act3(self.conv3(out)))
        out = self.flatten(out)
        out = self.sigmoid(self.fc1(out)).squeeze()
        return out

    @property
    def hyperparameters(self) -> dict:
        return self.hyperparams

    @property
    def criterion(self) -> nn.Module:
        return nn.BCELoss()

    @property
    def optimizer(self) -> optim.Optimizer:
        return self.optimizer_class(self.parameters(), lr=self.learning_rate)
